# ğŸ¯ RAGAS Evaluation - Quick Reference

## âš¡ Quick Start (3 bÆ°á»›c)

```bash
# 1. CÃ i Ä‘áº·t
pip install ragas datasets pandas

# 2. Generate predictions  
python generate_predictions.py

# 3. Evaluate
python evaluate_ragas.py
```

## ğŸ“ Files

| File | MÃ´ táº£ |
|------|-------|
| `data/Eveluate.json` | Input: CÃ¢u há»i + ground truth |
| `generate_predictions.py` | Script bÆ°á»›c 1: Generate contexts + answer |
| `data/predictions.json` | Output bÆ°á»›c 1: CÃ³ Ä‘á»§ 4 trÆ°á»ng |
| `evaluate_ragas.py` | Script bÆ°á»›c 2: ÄÃ¡nh giÃ¡ RAGAS |
| `evaluation_results.json` | Output: RAGAS scores |
| `evaluation_results.csv` | Output: Báº£ng dá»¯ liá»‡u |
| `RAGAS_GUIDE.md` | HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ |

## ğŸ¯ RAGAS Metrics

| Metric | Äo lÆ°á»ng gÃ¬? | Äiá»ƒm tá»‘t |
|--------|--------------|----------|
| Faithfulness | Trung thá»±c vá»›i contexts | > 0.7 |
| Answer Relevancy | LiÃªn quan Ä‘áº¿n cÃ¢u há»i | > 0.7 |
| Context Precision | Cháº¥t lÆ°á»£ng contexts | > 0.7 |
| Context Recall | Äá»§ thÃ´ng tin chÆ°a | > 0.7 |
| Answer Similarity | Giá»‘ng ground truth | > 0.7 |
| Answer Correctness | Tá»•ng há»£p | > 0.7 |

## ğŸ’» Commands

### Generate predictions
```bash
# Basic
python generate_predictions.py

# Custom files
python generate_predictions.py --input data/test.json --output data/test_pred.json
```

### Evaluate
```bash
# All metrics
python evaluate_ragas.py

# Specific metrics
python evaluate_ragas.py --metrics faithfulness answer_correctness

# Custom files
python evaluate_ragas.py --input data/test_pred.json --output results.json
```

## ğŸ“Š Output Format

### predictions.json
```json
{
  "question": "...",
  "ground_truth": "...",
  "contexts": ["...", "...", "..."],
  "answer": "..."
}
```

### evaluation_results.json
```json
{
  "ragas_scores": {
    "faithfulness": 0.8234,
    "answer_relevancy": 0.7891,
    ...
  },
  "predictions": [...],
  "summary": {...}
}
```

## âŒ Troubleshooting

| Lá»—i | Fix |
|-----|-----|
| No index found | `python main.py` |
| Import ragas failed | `pip install ragas datasets pandas` |
| Empty contexts | Rebuild index: `python main.py` |
| Rate limit | Wait hoáº·c reduce requests |

## ğŸ“ˆ Cáº£i thiá»‡n Ä‘iá»ƒm sá»‘

| Metric tháº¥p | CÃ¡ch cáº£i thiá»‡n |
|-------------|----------------|
| Faithfulness | Improve contexts quality |
| Answer Relevancy | Tune LLM prompt |
| Context Precision/Recall | Optimize retrieval (chunk size, top_k) |
| Answer Correctness | Improve contexts + prompt |

## ğŸ”„ Workflow

```
Eveluate.json
    â†“
generate_predictions.py  â† BÆ°á»›c 1 (cháº¡y 1 láº§n)
    â†“
predictions.json
    â†“
evaluate_ragas.py        â† BÆ°á»›c 2 (cÃ³ thá»ƒ cháº¡y nhiá»u láº§n)
    â†“
evaluation_results.json
```

## ğŸ’¡ Pro Tips

âœ… Test nhá» trÆ°á»›c: Táº¡o file 10-20 cÃ¢u Ä‘á»ƒ test

âœ… Backup predictions: KhÃ´ng pháº£i generate láº¡i

âœ… Compare versions: LÆ°u results vá»›i tÃªn khÃ¡c nhau

âœ… Track experiments: Document changes vÃ  scores

## ğŸ“š Documentation

- **RAGAS_GUIDE.md**: HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§
- **RAGAS_SUMMARY.md**: TÃ³m táº¯t há»‡ thá»‘ng
- **predictions_example.json**: VÃ­ dá»¥ output

---

**Need help?** â†’ Äá»c `RAGAS_GUIDE.md`
